{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to datasets\n",
    "video_path = '../data/ZJ-videos'\n",
    "alphabet_path = '../data/mnist_asl_alphabet_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of frames to pad to\n",
    "selected_frame_dim = 180  # Example value\n",
    "padding_value = torch.zeros((1, 21, 3), dtype=torch.float32)  # Padding value\n",
    "\n",
    "# Target size for resizing frames and images\n",
    "target_size = (224, 224)\n",
    "\n",
    "# Label mapping\n",
    "label_mapping = {'J': 0, 'Z': 1, 'nothing': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721164616.508004  783115 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1721164616.525374  783174 gl_context.cc:357] GL version: 3.1 (OpenGL ES 3.1 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: D3D12 (NVIDIA GeForce RTX 3060)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment an image\n",
    "def augment_image(image, flip=False):\n",
    "    # Random rotation\n",
    "    angle = random.uniform(-15, 15)\n",
    "    height, width = image.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((width // 2, height // 2), angle, 1)\n",
    "    rotated = cv2.warpAffine(image, M, (width, height))\n",
    "    \n",
    "    # Flip horizontally if specified\n",
    "    if flip:\n",
    "        rotated = cv2.flip(rotated, 1)\n",
    "    \n",
    "    return rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list of tuples of videos, with their corresponding label\n",
    "def load_videos(path, label):\n",
    "    video_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.avi')]\n",
    "    video_files.sort()\n",
    "    return [(f, label_mapping[label]) for f in video_files]\n",
    "\n",
    "# Returns list of tuples of images, with their corresponding label\n",
    "def load_images(path, labels):\n",
    "    image_files = []\n",
    "    for label in labels:\n",
    "        files = [os.path.join(path, label, f) for f in os.listdir(os.path.join(path, label)) if f.endswith('.jpg')]\n",
    "        files = random.sample(files, 12)  # Take 12 images per label\n",
    "        image_files.extend([(f, label_mapping['nothing']) for f in files])  # Label all images as 'nothing'\n",
    "    return image_files\n",
    "\n",
    "# [(x, y), (x, y),..,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract landmarks from a video file\n",
    "def extract_landmarks_from_video(video_file, target_size=(224, 224), selected_frame_dim=180):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frames = []\n",
    "    corrupted_video = False\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, target_size)  # Resize frame\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                frame_landmarks = torch.tensor([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark], dtype=torch.float32)\n",
    "                frames.append(frame_landmarks.unsqueeze(0))  # Add a batch dimension\n",
    "        else:\n",
    "            frames.append(torch.zeros((1, 21, 3), dtype=torch.float32))  # If no hand detected, append zero landmarks\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Check for video corruption due to missing landmarks\n",
    "    if len(frames) == 0 or not all([torch.any(frame != 0) for frame in frames]):\n",
    "        corrupted_video = True\n",
    "\n",
    "    # Pad or trim frames to the selected frame dimension\n",
    "    padded_frames_tensor = pad_sequence(frames, selected_frame_dim, padding_value=torch.zeros((1, 21, 3), dtype=torch.float32))\n",
    "\n",
    "    return padded_frames_tensor, corrupted_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract landmarks from an image\n",
    "def extract_landmarks_from_image(image_file, target_size=(224, 224)):\n",
    "    image = cv2.imread(image_file)\n",
    "    if image is None:\n",
    "        return None, True  # Mark as corrupted if image cannot be read\n",
    "    \n",
    "    image = cv2.resize(image, target_size)  # Resize image\n",
    "    augmented_image = augment_image(image)  # Apply augmentation\n",
    "    image_rgb = cv2.cvtColor(augmented_image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            return torch.tensor([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark], dtype=torch.float32), False\n",
    "    else:\n",
    "        return torch.zeros((1, 21, 3), dtype=torch.float32), True  # If no hand detected, return zero landmarks and mark as corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad or trim frames to a specified length\n",
    "def pad_sequence(sequence, target_length, padding_value):\n",
    "    padded_sequence = []\n",
    "    for frame in sequence:\n",
    "        padded_sequence.append(frame)\n",
    "    while len(padded_sequence) < target_length:\n",
    "        padded_sequence.append(padding_value)\n",
    "    return torch.cat(padded_sequence[:target_length], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process videos\n",
    "def process_videos(video_files):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for video_file, label in tqdm(video_files, desc='Processing videos'):\n",
    "        landmarks, is_corrupted = extract_landmarks_from_video(video_file)\n",
    "        if not is_corrupted and landmarks is not None:\n",
    "            data.append(landmarks)\n",
    "            labels.append(label)\n",
    "    return torch.stack(data), torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "# Process images\n",
    "def process_images(image_files):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for image_file, label in tqdm(image_files, desc='Processing images'):\n",
    "        landmarks, is_corrupted = extract_landmarks_from_image(image_file)\n",
    "        if not is_corrupted and landmarks is not None:\n",
    "            data.append(landmarks)\n",
    "            labels.append(label)\n",
    "    return torch.stack(data), torch.tensor(labels, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ../data/ZJ-videos/j/24.avi is corrupted or has no valid landmarks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x8b54000] overread 8\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "video_file = '../data/ZJ-videos/j/24.avi'\n",
    "landmarks, is_corrupted = extract_landmarks_from_video(video_file)\n",
    "\n",
    "if not is_corrupted and landmarks is not None:\n",
    "    # Ensure landmarks are tensors\n",
    "    landmarks_tensor = torch.tensor(landmarks, dtype=torch.float32)\n",
    "    print(f\"Successfully extracted landmarks from video: {video_file}\")\n",
    "    print(f\"Shape of landmarks tensor: {landmarks_tensor.shape}\")\n",
    "else:\n",
    "    print(f\"Video {video_file} is corrupted or has no valid landmarks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j/24.avi is not valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:   1%|▌                                                               | 5/569 [00:04<08:17,  1.13it/s][mjpeg @ 0x9229680] overread 8\n",
      "Processing videos:   1%|▉                                                               | 8/569 [00:06<07:19,  1.28it/s][mjpeg @ 0x9305b40] overread 8\n",
      "Processing videos:   2%|█▎                                                             | 12/569 [00:09<07:34,  1.22it/s][mjpeg @ 0x8b71e40] overread 8\n",
      "Processing videos:   2%|█▌                                                             | 14/569 [00:10<07:03,  1.31it/s][mjpeg @ 0x9311180] overread 8\n",
      "Processing videos:   5%|███▏                                                           | 29/569 [00:25<08:49,  1.02it/s][mjpeg @ 0x9342840] overread 8\n",
      "Processing videos:   5%|███▍                                                           | 31/569 [00:26<07:31,  1.19it/s][mjpeg @ 0x91fd640] overread 8\n",
      "Processing videos:   7%|████                                                           | 37/569 [00:31<07:49,  1.13it/s][mjpeg @ 0x92db940] overread 8\n",
      "Processing videos:   7%|████▎                                                          | 39/569 [00:32<06:50,  1.29it/s][mjpeg @ 0x8b6a340] overread 8\n",
      "Processing videos:   7%|████▍                                                          | 40/569 [00:33<05:42,  1.54it/s][mjpeg @ 0x9304c40] overread 8\n",
      "Processing videos:   8%|████▉                                                          | 45/569 [00:37<07:54,  1.10it/s][mjpeg @ 0x88e1880] overread 8\n",
      "Processing videos:   9%|█████▌                                                         | 50/569 [00:41<07:08,  1.21it/s][mjpeg @ 0x921e580] overread 8\n",
      "Processing videos:  10%|██████                                                         | 55/569 [00:45<07:32,  1.14it/s][mjpeg @ 0x944dd00] overread 8\n",
      "Processing videos:  10%|██████▎                                                        | 57/569 [00:46<06:40,  1.28it/s][mjpeg @ 0x944dd00] overread 1\n",
      "Processing videos:  10%|██████▍                                                        | 58/569 [00:46<05:36,  1.52it/s][mjpeg @ 0x944dd00] overread 3\n",
      "Processing videos:  12%|███████▎                                                       | 66/569 [00:54<07:41,  1.09it/s][mjpeg @ 0x920bbc0] overread 3\n",
      "Processing videos:  12%|███████▊                                                       | 70/569 [00:57<07:08,  1.16it/s][mjpeg @ 0x88e5440] overread 8\n",
      "Processing videos:  15%|█████████▋                                                     | 87/569 [01:11<06:49,  1.18it/s][mjpeg @ 0x944dd00] overread 8\n",
      "Processing videos:  17%|██████████▊                                                    | 98/569 [01:21<07:26,  1.06it/s][mjpeg @ 0x9252f00] overread 8\n",
      "Processing videos:  18%|██████████▉                                                   | 100/569 [01:23<06:34,  1.19it/s][mjpeg @ 0x944dd00] overread 8\n",
      "Processing videos:  21%|█████████████                                                 | 120/569 [01:42<08:32,  1.14s/it][mjpeg @ 0x933c640] overread 8\n",
      "Processing videos:  21%|█████████████▏                                                | 121/569 [01:43<07:29,  1.00s/it][mjpeg @ 0x933c640] overread 8\n",
      "Processing videos:  22%|█████████████▉                                                | 128/569 [01:49<07:08,  1.03it/s][mjpeg @ 0x92d9200] overread 7\n",
      "Processing videos:  23%|██████████████▏                                               | 130/569 [01:51<05:53,  1.24it/s][mjpeg @ 0x92feb40] overread 8\n",
      "Processing videos:  23%|██████████████▍                                               | 133/569 [01:53<05:56,  1.22it/s][mjpeg @ 0x9200280] overread 8\n",
      "Processing videos:  24%|██████████████▊                                               | 136/569 [01:55<05:53,  1.22it/s][mjpeg @ 0x9200280] overread 8\n",
      "Processing videos:  24%|███████████████▏                                              | 139/569 [01:57<05:41,  1.26it/s][mjpeg @ 0x962ea00] overread 8\n",
      "Processing videos:  25%|███████████████▌                                              | 143/569 [02:01<05:46,  1.23it/s][mjpeg @ 0x88e23c0] overread 8\n",
      "Processing videos:  25%|███████████████▊                                              | 145/569 [02:02<05:34,  1.27it/s][mjpeg @ 0x88e23c0] overread 8\n",
      "Processing videos:  28%|█████████████████▏                                            | 158/569 [02:15<06:44,  1.02it/s]"
     ]
    }
   ],
   "source": [
    "# Load and process datasets\n",
    "video_files = load_videos(os.path.join(video_path, 'j'), 'J') + load_videos(os.path.join(video_path, 'z'), 'Z')\n",
    "image_files = load_images(alphabet_path, list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))\n",
    "\n",
    "train_videos, val_videos = train_test_split(video_files, test_size=0.2, random_state=42)\n",
    "train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "train_video_data, train_video_labels = process_videos(train_videos)\n",
    "val_video_data, val_video_labels = process_videos(val_videos)\n",
    "train_image_data, train_image_labels = process_images(train_images)\n",
    "val_image_data, val_image_labels = process_images(val_images)\n",
    "\n",
    "train_data = torch.cat((train_video_data, train_image_data), dim=0)\n",
    "train_labels = torch.cat((train_video_labels, train_image_labels), dim=0)\n",
    "val_data = torch.cat((val_video_data, val_image_data), dim=0)\n",
    "val_labels = torch.cat((val_video_labels, val_image_labels), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "torch.save((train_data, train_labels), 'train_data.pt')\n",
    "torch.save((val_data, val_labels), 'val_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train data\n",
    "def visualize_data(data, labels, num_examples=50):\n",
    "    # Select a random example from the data\n",
    "    random_idx = random.randint(0, data.shape[0] - 1)\n",
    "    selected_data = data[random_idx][:num_examples]  # Take the first 50 frames\n",
    "    selected_label = labels[random_idx].item()\n",
    "\n",
    "    # Print the respective label\n",
    "    print(f\"Label for selected example: {selected_label}\")\n",
    "\n",
    "    # Plot the hand landmarks for each frame\n",
    "    fig, axes = plt.subplots(5, 10, figsize=(20, 10))\n",
    "    fig.suptitle(f'Hand Landmarks for First {num_examples} Frames - Label: {selected_label}', fontsize=16)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i >= selected_data.shape[0]:\n",
    "            break\n",
    "        ax.scatter(selected_data[i, :, 0], selected_data[i, :, 1], c='b', marker='o')\n",
    "        for j in range(selected_data.shape[1]):\n",
    "            ax.text(selected_data[i, j, 0], selected_data[i, j, 1], str(j), fontsize=9)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f'Frame {i+1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize train data\n",
    "visualize_data(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    return torch.load(filename)\n",
    "\n",
    "# Load data\n",
    "train_data, train_labels = load_data('train_data.pt')\n",
    "val_data, val_labels = load_data('val_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random example from the training data\n",
    "random_idx = random.randint(0, train_data.shape[0] - 1)\n",
    "selected_data = train_data[random_idx][:50]  # Take the first 50 frames\n",
    "selected_label = train_labels[random_idx].item()\n",
    "\n",
    "# Print the respective label\n",
    "print(f\"Label for selected example: {selected_label}\")\n",
    "\n",
    "# Plot the hand landmarks for each frame\n",
    "fig, axes = plt.subplots(5, 10, figsize=(20, 10))\n",
    "fig.suptitle(f'Hand Landmarks for First 50 Frames - Label: {selected_label}', fontsize=16)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i >= selected_data.shape[0]:\n",
    "        break\n",
    "    ax.scatter(selected_data[i, :, 0], selected_data[i, :, 1], c='b', marker='o')\n",
    "    for j in range(selected_data.shape[1]):\n",
    "        ax.text(selected_data[i, j, 0], selected_data[i, j, 1], str(j), fontsize=9)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f'Frame {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging functions\n",
    "def extract_landmarks_from_video(video_file, target_size=(224, 224), max_frames=50):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frames = []\n",
    "    landmarks = []\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, target_size)  # Resize frame\n",
    "        frames.append(frame)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(frame_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                frame_landmarks = [[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]\n",
    "                landmarks.append(frame_landmarks)\n",
    "        else:\n",
    "            landmarks.append([[0, 0, 0]] * 21)  # If no hand detected, append zero landmarks\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    return frames, landmarks\n",
    "\n",
    "# Function to normalize landmarks based on image dimensions\n",
    "def normalize_landmarks(landmarks, image_width, image_height):\n",
    "    normalized_landmarks = []\n",
    "    for frame_landmarks in landmarks:\n",
    "        normalized_frame_landmarks = [[lm[0] * image_width, lm[1] * image_height, lm[2]] for lm in frame_landmarks]\n",
    "        normalized_landmarks.append(normalized_frame_landmarks)\n",
    "    return normalized_landmarks\n",
    "\n",
    "# Function to visualize frames\n",
    "def visualize_frames(frames):\n",
    "    fig, axes = plt.subplots(5, 10, figsize=(20, 10))\n",
    "    fig.suptitle('Frames', fontsize=16)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i >= len(frames):\n",
    "            break\n",
    "        ax.imshow(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f'Frame {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize landmarks separately\n",
    "def visualize_landmarks(frames, landmarks):\n",
    "    image_height, image_width, _ = frames[0].shape\n",
    "    normalized_landmarks = normalize_landmarks(landmarks, image_width, image_height)\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 10, figsize=(20, 10))\n",
    "    fig.suptitle('Hand Landmarks for First 50 Frames', fontsize=16)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i >= len(normalized_landmarks):\n",
    "            break\n",
    "        ax.scatter([lm[0] for lm in normalized_landmarks[i]], [lm[1] for lm in normalized_landmarks[i]], c='b', marker='o')\n",
    "        for j in range(len(normalized_landmarks[i])):\n",
    "            ax.text(normalized_landmarks[i][j][0], normalized_landmarks[i][j][1], str(j), fontsize=9)\n",
    "        ax.set_xlim(0, image_width)\n",
    "        ax.set_ylim(0, image_height)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f'Frame {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Path to the video file\n",
    "video_file_path = '../data/ZJ-videos/j/25.avi'  # Update this path\n",
    "\n",
    "# Extract and visualize the first 50 frames and landmarks\n",
    "frames, landmarks = extract_landmarks_from_video(video_file_path)\n",
    "\n",
    "# Visualize frames\n",
    "visualize_frames(frames)\n",
    "\n",
    "# Visualize landmarks\n",
    "visualize_landmarks(frames, landmarks)\n",
    "\n",
    "# MediaPipe coordinates are normalized between 0 and 1, based on the grid size"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 23079,
     "sourceId": 29550,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3428198,
     "sourceId": 5980815,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (deep-learning)",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
