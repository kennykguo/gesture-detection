{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcd51f4-c447-4f6a-b22b-8500b181f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c3e50c-7411-46b0-88e6-2a690051e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for label, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            # Double check that we need to convert to grayscale from paper\n",
    "            image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "            image = image.resize((64, 64))\n",
    "            images.append(np.array(image))\n",
    "            labels.append(label)\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f4a386-68b8-44c0-8564-e71adae68c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(images, labels, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    assert train_ratio + val_ratio + test_ratio == 1, \"Ratios must sum to 1\"\n",
    "    total_size = len(labels)\n",
    "    indices = list(range(total_size))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    train_split = int(train_ratio * total_size)\n",
    "    val_split = int(val_ratio * total_size)\n",
    "    \n",
    "    train_indices = indices[:train_split]\n",
    "    val_indices = indices[train_split:train_split + val_split]\n",
    "    test_indices = indices[train_split + val_split:]\n",
    "    \n",
    "    train_images = images[train_indices]\n",
    "    train_labels = labels[train_indices]\n",
    "    val_images = images[val_indices]\n",
    "    val_labels = labels[val_indices]\n",
    "    test_images = images[test_indices]\n",
    "    test_labels = labels[test_indices]\n",
    "    \n",
    "    return train_images, train_labels, val_images, val_labels, test_images, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d04e385-7f47-41bf-8411-5ae6652d3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/mnist_asl_alphabet_train'\n",
    "\n",
    "images, labels = load_data(data_dir)\n",
    "\n",
    "train_images, train_labels, val_images, val_labels, test_images, test_labels = split_data(images, labels)\n",
    "\n",
    "# Convert to tensors (unsqueeze for Conv2d and normalize)\n",
    "train_images = torch.tensor(train_images, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "val_images = torch.tensor(val_images, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "test_images = torch.tensor(test_images, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06135bf1-70f8-414a-923c-f88b7988076e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b55992-5af7-4bfc-bb60-e3e9d0200ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dbf96b-92b9-4433-99e4-ec96d88e8fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59620aa-6d64-48b5-ba4f-45c6fbfabf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ASLModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 29)  # 29 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 16 * 16)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1044c-791b-49b0-aea8-06b2e8e00c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ASLModel().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fab8fc-ea66-44cc-b541-c212dea60b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(train_images.size()[0])\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Loop over all batches, in terms of batch_size\n",
    "    for i in range(0, train_images.size()[0], batch_size):\n",
    "        \n",
    "        # Get the random permutation of numbers according to batch_size\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        \n",
    "        # Get the corresponding images and labels \n",
    "        batch_images, batch_labels = train_images[indices], train_labels[indices]\n",
    "\n",
    "        # Bring them to the corresponding device\n",
    "        batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
    "        \n",
    "        outputs = model(batch_images)\n",
    "        loss = loss_fn(outputs, batch_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_images.size(0)\n",
    "\n",
    "        print(loss)\n",
    "    \n",
    "    epoch_loss = running_loss / train_images.size(0)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(val_images.to(device))\n",
    "        val_loss = criterion(val_outputs, val_labels.to(device))\n",
    "        print(f'Validation Loss: {val_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d83e50-7b68-4bbc-a682-34bb898e7b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model with test dataset\n",
    "def evaluate_model(test_images, test_labels):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        test_outputs = model(test_images.to(device))\n",
    "        \n",
    "        _, preds = torch.max(test_outputs, 1)\n",
    "        \n",
    "        accuracy = accuracy_score(test_labels.cpu(), preds.cpu())\n",
    "        \n",
    "        precision = precision_score(test_labels.cpu(), preds.cpu(), average='macro')\n",
    "        \n",
    "        recall = recall_score(test_labels.cpu(), preds.cpu(), average='macro')\n",
    "        \n",
    "    return accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c220d2a-d6c7-4f72-b55d-bd9c60c7663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy, precision, recall = evaluate_model(model, test_images, test_labels)\n",
    "print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4a465-1f4e-4e3e-9a89-664b9994d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state(dict, \"model.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning)",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
